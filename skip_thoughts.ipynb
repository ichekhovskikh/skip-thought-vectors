{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skip-thoughts",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/ichekhovskikh/skip-thought-vectors/blob/master/skip_thoughts.ipynb",
      "authorship_tag": "ABX9TyO69y2uAB420UfGsO54BEtD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ichekhovskikh/skip-thought-vectors/blob/master/skip_thoughts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNlBN9L9--GG",
        "colab_type": "text"
      },
      "source": [
        "# Подготовка\n",
        "\n",
        "Установим флаг floatX=float32, чтобы избежать ошибок типа TypeError."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qauJ0E_8-sb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo -e \"\\n[global]\\nfloatX=float32\\n\" >> ~/.theanorc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_eIsNhu_xKu",
        "colab_type": "text"
      },
      "source": [
        "Необходимо подлючить Google Drive, где по пути skip-thoughts-master/training располагается библиотека для работы со Skip Throughts Vectors. Папку training можно скачать из репозитория https://github.com/ichekhovskikh/skip-thought-vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyz9TC-oQgu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXJz0dqR_2Xd",
        "colab_type": "text"
      },
      "source": [
        "Сделать импорт библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WetwZIyMQdzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import importlib.util\n",
        "import sys\n",
        "\n",
        "sys.path.append('/content/drive/My Drive/skip-thoughts-master/training')\n",
        "import vocab\n",
        "import train\n",
        "import tools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGRN0p7pAMbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pymorphy2[fast]\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import os\n",
        "import collections\n",
        "import smart_open\n",
        "import random\n",
        "import json\n",
        "import urllib.request\n",
        "import pymorphy2\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk import tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from pymystem3 import Mystem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZYD5AXcNYRU",
        "colab_type": "text"
      },
      "source": [
        "#Начинаем\n",
        "Для начала нам понадобится комплект документов для обучения нашей модели Skip Thought Vectors. Теоретически, документ может быть чем угодно: коротким твитом из 140 символов, отдельным абзацем, новостной статьей или книгой. В NLP комплект документов часто называют корпусом.\n",
        "\n",
        "Будем тренировать нашу модель на собственном корпусе. Этот корпус содержит 500 научных статей на 5 различных тем.\n",
        "\n",
        "Для тестирования будет использоваться тестовый корпус из 50 статей (10 статей на каждую тему).\n",
        "\n",
        "Dataset состоит из трех строк: id (идентификатор строки), text (текст статьи), tag (идентификатор самой статьи, вектор которого будем обучать), class_name (класс текста, был размечен вручную, необходим только для тестирования)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KR5mKmSVAc-L",
        "colab": {}
      },
      "source": [
        "#@title Введите путь к файлам исходной базы статей:\n",
        "train_path = 'https://raw.githubusercontent.com/ichekhovskikh/cyberleninka-article-downloader/master/train_corpus.json' #@param {type: \"string\"}\n",
        "test_path = 'https://raw.githubusercontent.com/ichekhovskikh/cyberleninka-article-downloader/master/test_corpus.json' #@param {type: \"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzY-WMJjJhtR",
        "colab_type": "text"
      },
      "source": [
        "## Опредлим функцию для чтения и предварительной обработки текста\n",
        "Ниже мы определяем функцию для открытия  train/test файла, предварительно обрабатываем каждый текст датасета, используя простой инструмент предварительной обработки gensim (то есть, разбиваем текст на отдельные слова, удалите знаки препинания, установите строчные буквы и т. д.), лемматизацию, удаление стоп слов и возвращаем список слов. Для обучения модели нам нужно будет связать тег с каждым документом учебного корпуса. В нашем случае тег - это идентификатор статьи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFO_znw317kM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def lemmatize(words):\n",
        "    for word in words:\n",
        "        yield morph.parse(word)[0].normal_form"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr6Qz8Ucw_ee",
        "colab_type": "text"
      },
      "source": [
        "Удаление стоп слов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRm8tmSe17Hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    return [word for word in words if word not in russian_stopwords]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB82kEYi-nek",
        "colab_type": "text"
      },
      "source": [
        "Предобработка текста статьи:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHwqCiuN-hDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def advanced_preprocess(text):\n",
        "    normalized_text = gensim.utils.simple_preprocess(text)\n",
        "    normalized_text = list(lemmatize(normalized_text))\n",
        "    normalized_text = remove_stopwords(normalized_text)\n",
        "    return normalized_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5agV69PVxEIl",
        "colab_type": "text"
      },
      "source": [
        "Отрытие файла с корпусом статей:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMO4zHrqGTws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_corpus(corpus_path):\n",
        "    with urllib.request.urlopen(corpus_path) as corpus_url:\n",
        "        corpus = json.loads(corpus_url.read().decode())\n",
        "        for article in corpus:\n",
        "            sentences = tokenize.sent_tokenize(article['text'], 'russian')\n",
        "            normalized_sentences = []\n",
        "            for sentence in sentences:\n",
        "                normalized_sentences.append(' '.join(advanced_preprocess(sentence)))\n",
        "            yield normalized_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_nR03Khn6Lds"
      },
      "source": [
        "Получение исходного текста статьи по индексу:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1gSBZ1G1BNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_article_text_by_index(index, corpus_path):\n",
        "    with urllib.request.urlopen(corpus_path) as corpus_url:\n",
        "        corpus = json.loads(corpus_url.read().decode())\n",
        "        return corpus[index]['text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsDE0caoRrMN",
        "colab_type": "text"
      },
      "source": [
        "Получение категории статьи по индексу:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhMV7VB-Rn-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_article_class_by_index(index, corpus_path):\n",
        "    with urllib.request.urlopen(corpus_path) as corpus_url:\n",
        "        corpus = json.loads(corpus_url.read().decode())\n",
        "        return corpus[index]['class_name']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui3oWTnrxKNt",
        "colab_type": "text"
      },
      "source": [
        "Загружаем корпуса:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0vJrTipGemC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_corpus = list(read_corpus(train_path))\n",
        "test_corpus = list(read_corpus(test_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKA3sFQxKYJP",
        "colab_type": "text"
      },
      "source": [
        "Давайте посмотрим на учебный и тестовый корпуса:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9aPeDPvGgve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_corpus[:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29upY6eyKlmk",
        "colab_type": "text"
      },
      "source": [
        "Корпус тестирования выглядит так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrWLt9Z8Gm66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(test_corpus[:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qagqjk0DSpLa",
        "colab_type": "text"
      },
      "source": [
        "Создайте три пустых файла в Google Drive для сохранения в них словаря, модели Word2Vec, модели Skip Throughts Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8mvv1b-TB5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_dictionary = '/content/drive/My Drive/sample_data/vocab.pkl'\n",
        "path_to_word2vec = '/content/drive/My Drive/sample_data/word2vec.bin'\n",
        "path_to_model = '/content/drive/My Drive/sample_data/model.npz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-cTR8Uz7JYp",
        "colab_type": "text"
      },
      "source": [
        "# Обучение модели\n",
        "## Создание словаря\n",
        "Состовим словарь из тренеровочного корпуса предложений и сохраним его по пути path_to_dictionary для дальнейшего использования"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu0bGe_WXYTF",
        "colab_type": "text"
      },
      "source": [
        "Объединение всех предложений в один список"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmiteWv3VhOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flatten(text):\n",
        "    return [item for sublist in text for item in sublist]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGBpE5hXXmdP",
        "colab_type": "text"
      },
      "source": [
        "Создание и сохранение словаря"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqyQi72JT0KF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentenses = flatten(train_corpus)\n",
        "worddict, wordcount = vocab.build_dictionary(sentenses)\n",
        "vocab.save_dictionary(worddict, wordcount, path_to_dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-zg08E4X5Yc",
        "colab_type": "text"
      },
      "source": [
        " ## Создание модели Word2Vec\n",
        "Составим модель Word2Vec из тренеровочного корпуса предложений и сохраним его по пути path_to_word2vec для дальнейшего использования"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7rjlG1VaYGK",
        "colab_type": "text"
      },
      "source": [
        "Разделить предложение на слова"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG41WYwNY9t_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_tokenize(text):\n",
        "    flat_text = flatten(text)\n",
        "    return [item.split(' ') for item in flat_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP8UTYRvafOV",
        "colab_type": "text"
      },
      "source": [
        "Создание и сохранение модели Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6w0x7RlnzVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Укажите параметры обучения модели слов:\n",
        "vector_size = 300 #@param\n",
        "window =  5 #@param\n",
        "epochs =  100 #@param\n",
        "alpha = 0.001 #@param\n",
        "learning_method = \"skip-gram\" #@param [\"skip-gram\", \"CBOW\"] {type:\"raw\"}\n",
        "\n",
        "sg = 1\n",
        "if (learning_method == \"CBOW\"):\n",
        "    sg = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9duhzq_4YU1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = word_tokenize(train_corpus)\n",
        "%time model = Word2Vec(sentences = words, size = vector_size, window = window, alpha = alpha, sg=sg, iter=epochs)\n",
        "\n",
        "model.wv.save_word2vec_format(path_to_word2vec, binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sVINChchw9T",
        "colab_type": "text"
      },
      "source": [
        "# Создание модели Skip Throughts Vectors\n",
        "Создадим модель Skip Throughts Vectors для преобрахования текстов в вектора.\n",
        "Сохраним ее в Google Drive по пути path_to_model. Параметры обучения:\n",
        "- dim_word: размерность вектора слов в RNN сети\n",
        "- max_epochs: количество эпох обучения\n",
        "- displayFreq: отображение прогресса после такого количества эпох\n",
        "- learning_rate: коэффициент обучения\n",
        "- n_words: размерность словаря\n",
        "- maxlen_w: максимальное количество слов в предложении, если ниже заданной - игнорировать\n",
        "- saveto: путь для сохранения модели\n",
        "- dictionary: путь к словарю\n",
        "- saveFreq: частота сохранения модели в эпохах"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aoKVTgSS2sC1",
        "colab": {}
      },
      "source": [
        "#@title Укажите параметры обучения модели предложений:\n",
        "max_epochs = 100 #@param\n",
        "neurons_count = 2400 #@param\n",
        "learning_rate = 0.01 #@param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KvtLIZOiAxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentenses = flatten(train_corpus)\n",
        "%time train.trainer(X = sentenses, n_words = len(worddict) + 2, dim=neurons_count, learning_rate=learning_rate, max_epochs=max_epochs, dictionary = path_to_dictionary, saveto = path_to_model, saveFreq = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTcZvsQQqYTV",
        "colab_type": "text"
      },
      "source": [
        "Загрузка обученной модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFvH5FqpqEuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tools.load_model(\n",
        "    path_to_model = path_to_model,\n",
        "    path_to_dictionary = path_to_dictionary,\n",
        "    path_to_word2vec = path_to_word2vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OriWNZ5RiUVd",
        "colab_type": "text"
      },
      "source": [
        "# Поиск похожих текстов на основе обученной модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeuA40DoSi0x",
        "colab_type": "text"
      },
      "source": [
        "Обратная сглаженная частота (вес слова), где\n",
        "- a: гиперпараметр, который обычно равен 0.001 (конфигурация, внешняя по отношению к модели, значение которой невозможно оценить по данным):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCA8NOFtSZ0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = 0.001\n",
        "\n",
        "def sif(word, sentence):\n",
        "    frequency = sentence.count(word) / len(sentence)\n",
        "    return a / (a + frequency) \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyOLBTMAVVml",
        "colab_type": "text"
      },
      "source": [
        "Получить вектор текста:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp9jnjq1Zahc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def paragraph_vector(model, paragraph):\n",
        "    sentence_vectors = tools.encode(model, paragraph)\n",
        "    paragraph_vector = []\n",
        "    vector_count = 0\n",
        "    for sentence_vector in sentence_vectors:\n",
        "        if not paragraph_vector:\n",
        "            paragraph_vector = sentence_vector\n",
        "        else: \n",
        "            paragraph_vector = [x + y for x, y in zip(paragraph_vector, sentence_vector)]\n",
        "            vector_count += 1\n",
        "    if vector_count != 0:\n",
        "        paragraph_vector = [x / vector_count for x in paragraph_vector]\n",
        "    return paragraph_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbmpPBjkentu",
        "colab_type": "text"
      },
      "source": [
        "Вычислим вектора для всех статей исходной базы для ускорения работы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhs-1jl3exHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paragraph_vectors = []\n",
        "for text in train_corpus:\n",
        "    text_vector = paragraph_vector(model, text)\n",
        "    paragraph_vectors.append(text_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r56A1M-4g9y0",
        "colab_type": "text"
      },
      "source": [
        "Определение косинусной близости двух векторов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ8hxIVpStCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(first_vector, second_vector):\n",
        "    numerator = sum([x * y for x, y in zip(first_vector, second_vector)])\n",
        "    first_norm = pow(sum([x * x for x in first_vector]), 0.5)\n",
        "    second_norm = pow(sum([x * x for x in second_vector]), 0.5)\n",
        "    return numerator / (first_norm * second_norm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jak5jPaK6QmC",
        "colab_type": "text"
      },
      "source": [
        "Поиск похожих текстов из корпуса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omwDc4Vd6YIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sims(model, paragraph, count = 5):\n",
        "    result = []\n",
        "    compared_vector = paragraph_vector(model, paragraph)\n",
        "    for index, text_vector in enumerate(paragraph_vectors):\n",
        "        similarity = cosine_similarity(compared_vector, text_vector)\n",
        "        result.append([index, similarity])\n",
        "    return sorted(result,  key=lambda x: x[1], reverse=True)[:count]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSeGSybK4tpG",
        "colab_type": "text"
      },
      "source": [
        "# Оценочная модель\n",
        "Чтобы оценить нашу новую модель, мы сначала выведем новые векторы для каждого документа тренировочного корпуса, сравним выведенные векторы с тренировочным корпусом.\n",
        "\n",
        "Проверка выведенного вектора по обучающему вектору является своего рода «проверкой работоспособности» в отношении того, ведет ли модель себя адекватно, хотя и не является реальным значением «точности».\n",
        "\n",
        "Можем взглянуть на пример:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KendqFnt496K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_id = random.randint(0, len(train_corpus) - 1)\n",
        "doc = train_corpus[doc_id]\n",
        "sims = sims(model, doc)\n",
        "print('ТЕКСТ ИСХДНОГО ДОКУМЕНТА ({}): «{}»\\n'.format(doc_id, get_article_text_by_index(doc_id, train_path)))\n",
        "num = 0\n",
        "for index, similarity in sims:\n",
        "    num += 1\n",
        "    print(u'%s) %s: «%s»\\n' % (num, similarity, get_article_text_by_index(index, train_path)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eq8rXTHOwBQ",
        "colab_type": "text"
      },
      "source": [
        "# Тестирование модели\n",
        "Используя тот же подход, что и выше, мы выведем вектор для случайно выбранного тестового документа и сравним документ с нашей моделью с помощью f1-меры."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y7K6N4jLIXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "test_size = len(test_corpus)\n",
        "\n",
        "predicted_classes = [] \n",
        "test_classes = []\n",
        "\n",
        "for doc_index in range(test_size):\n",
        "    doc = test_corpus[doc_index]\n",
        "    similarities = sims(model, doc)\n",
        "    \n",
        "    test_article_class = get_article_class_by_index(doc_index, test_path)\n",
        "    print('ТЕКСТ ИСХДНОГО ДОКУМЕНТА ({}) «{}»: «{}»\\n'.format(doc_index, test_article_class, get_article_text_by_index(doc_index, test_path)))\n",
        "    num = 0\n",
        "    for index, similarity in similarities:\n",
        "        num += 1\n",
        "        predicted_article_class = get_article_class_by_index(index, train_path)\n",
        "        print(u'%s) %s «%s»: «%s»\\n' % (num, similarity, predicted_article_class, get_article_text_by_index(index, train_path)))\n",
        "        predicted_classes.append(predicted_article_class)\n",
        "        test_classes.append(test_article_class)\n",
        "        \n",
        "print('f1score: {}'.format(f1_score(test_classes, predicted_classes, average='macro')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91sOWCLiLOxS",
        "colab_type": "text"
      },
      "source": [
        "Тестирование на малых данных для выявления ошибок в ходе написания кода"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIgFPnpL6CBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = [\"первое предложение\", \"второе предложение\", \"третье предложение\", \"что-то еще предложение\"]\n",
        "text_split_on_words = [['первое', 'предложение'], ['второе', 'предложение'], ['третье', 'предложение'], ['что-то', 'еще', 'предложение']]\n",
        "\n",
        "worddict, wordcount = vocab.build_dictionary(text)\n",
        "vocab.save_dictionary(worddict, wordcount, path_to_dictionary)\n",
        "\n",
        "model = Word2Vec(text_split_on_words, size=300, window=5, min_count=1, workers=4)\n",
        "model.wv.save_word2vec_format(path_to_word2vec, binary=True)\n",
        "\n",
        "train.trainer(\n",
        "    text, \n",
        "    n_words=len(worddict) + 2,\n",
        "    dictionary=path_to_dictionary,\n",
        "    saveto=path_to_model,\n",
        "    saveFreq=1)\n",
        "\n",
        "model = tools.load_model(\n",
        "    path_to_model = path_to_model,\n",
        "    path_to_dictionary = path_to_dictionary',\n",
        "    path_to_word2vec = path_to_word2vec)\n",
        "\n",
        "vectors = tools.encode(model, text)\n",
        "print(cosine_similarity(vectors[0], vectors[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BSpkRxWUrC9B"
      },
      "source": [
        "# Поиск похожих научных документов\n",
        "Выполните поиск похожих научных статей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrflInTzrgmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Укажите путь к тексту статьи в формате *.txt или введите текст статьи:\n",
        "article_text = '' #@param {type: \"string\"}\n",
        "article_path = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/PY0101EN/labs/example1.txt' #@param {type: \"string\"}\n",
        "\n",
        "if (article_text == ''):\n",
        "    with urllib.request.urlopen(article_path) as article_url:\n",
        "      article_text = article_url.read().decode()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6JqjIFpu46v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalized_text = advanced_preprocess(article_text)\n",
        "sims = sims(model, normalized_text)\n",
        "\n",
        "print('ТЕКСТ ИСХДНОГО ДОКУМЕНТА «{}»\\n'.format(article_text)\n",
        "num = 0\n",
        "for index, similarity in sims:\n",
        "    num += 1\n",
        "    print(u'%s) %s: «%s»\\n' % (num, similarity, get_article_text_by_index(index, train_path)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}